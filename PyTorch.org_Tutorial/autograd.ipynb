{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background \n",
    "- Neural Networks (NN) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors\n",
    "- Training a NN happens in 2 steps:\n",
    "    + Forward Propagation\n",
    "        In forward prop, the NN makes its best guest about the correct output. It runs the input data through eaech of its functions to make this guess\n",
    "    + Backward Propagation\n",
    "        In backdrop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collection the derivatives of the error with respect to the parameters of the functions (gradient), and optimizing the parameters using gradient decent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage in PyTorch\n",
    "- Look at an example in a single training step\n",
    "- Load a pretrained resnet18 model from `torchvision`\n",
    "- Create a random data tensor to represent a single image with 3 channels, and height & width of 64, and its corresponding label initialized to some random values.\n",
    "- Label in pretrained model has shape (1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.5421, 0.2225, 0.1813,  ..., 0.3353, 0.7534, 0.0658],\n",
      "          [0.8229, 0.3776, 0.8601,  ..., 0.0821, 0.6289, 0.3139],\n",
      "          [0.9230, 0.1914, 0.3773,  ..., 0.5510, 0.6882, 0.9290],\n",
      "          ...,\n",
      "          [0.2035, 0.5775, 0.3468,  ..., 0.9233, 0.9648, 0.8529],\n",
      "          [0.2178, 0.2000, 0.3380,  ..., 0.4139, 0.0227, 0.2314],\n",
      "          [0.0952, 0.9879, 0.1932,  ..., 0.6048, 0.3325, 0.3075]],\n",
      "\n",
      "         [[0.2756, 0.7602, 0.1380,  ..., 0.1315, 0.5301, 0.7083],\n",
      "          [0.9754, 0.9447, 0.0938,  ..., 0.0809, 0.3266, 0.1984],\n",
      "          [0.2307, 0.4279, 0.7401,  ..., 0.7288, 0.8550, 0.7953],\n",
      "          ...,\n",
      "          [0.3426, 0.0096, 0.3271,  ..., 0.1005, 0.5864, 0.9310],\n",
      "          [0.5460, 0.3109, 0.3677,  ..., 0.3936, 0.7002, 0.0259],\n",
      "          [0.8847, 0.4800, 0.4041,  ..., 0.1071, 0.3233, 0.7646]],\n",
      "\n",
      "         [[0.5845, 0.0091, 0.6418,  ..., 0.9020, 0.8466, 0.7718],\n",
      "          [0.2247, 0.2234, 0.1239,  ..., 0.9381, 0.0825, 0.7746],\n",
      "          [0.0618, 0.4209, 0.6553,  ..., 0.5973, 0.8611, 0.3760],\n",
      "          ...,\n",
      "          [0.9724, 0.9184, 0.2634,  ..., 0.5780, 0.0967, 0.0300],\n",
      "          [0.9283, 0.8428, 0.3959,  ..., 0.1143, 0.4837, 0.9912],\n",
      "          [0.9103, 0.2428, 0.3772,  ..., 0.1680, 0.4798, 0.5827]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foward pass\n",
    "- Run input data through the model through each of its layers to make a prediction\n",
    "- This is a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-7.0983e-01, -2.7322e-01, -8.3016e-01, -1.4580e+00, -8.3909e-01,\n",
      "         -1.2968e-01, -4.6641e-01,  4.0165e-01,  3.7474e-01, -6.1585e-01,\n",
      "         -9.5696e-01, -6.7113e-01, -9.8432e-02, -1.0333e+00, -9.6147e-01,\n",
      "         -5.5149e-01, -6.5734e-01, -2.6695e-01, -5.6188e-01, -5.6120e-01,\n",
      "         -1.5620e+00, -1.0224e+00, -1.3473e+00, -6.2376e-04, -9.4227e-01,\n",
      "         -1.0288e+00, -6.2271e-01, -1.1116e+00, -7.0821e-01, -1.8653e-02,\n",
      "         -7.0968e-01, -8.2254e-01, -3.1741e-01, -6.6930e-01, -4.6288e-01,\n",
      "         -5.9411e-01,  5.6066e-01, -7.2004e-01, -3.5162e-01,  9.8255e-02,\n",
      "         -8.8806e-01, -7.0963e-01, -9.5301e-01, -6.9042e-02, -5.6950e-01,\n",
      "         -1.1889e-01, -1.0205e+00, -5.2649e-01, -1.1128e+00, -9.4764e-01,\n",
      "         -2.8345e-01,  4.0667e-01, -2.0549e-01, -6.6938e-01, -3.9741e-03,\n",
      "         -1.3550e+00, -1.3320e-01, -1.5202e+00, -3.3882e-01, -6.5573e-01,\n",
      "          7.7521e-01,  5.2745e-02,  5.5540e-02,  4.9108e-02, -9.5402e-01,\n",
      "         -1.7737e-01, -8.8296e-02, -1.1330e-01, -6.9148e-01, -9.2381e-01,\n",
      "         -1.2932e+00,  1.1614e-01, -1.5758e+00, -2.9140e-01, -1.0682e+00,\n",
      "         -1.0964e+00,  2.3282e-01, -4.7781e-01,  1.8445e-01, -2.6792e-02,\n",
      "         -7.3707e-01, -1.6465e+00, -1.0743e-01, -6.7903e-01, -6.0402e-01,\n",
      "         -2.3230e-01, -1.6900e-02,  9.4890e-02, -3.4255e-01, -9.9800e-01,\n",
      "         -1.1158e+00, -9.6663e-01, -2.0106e+00, -2.1751e-01, -1.1172e-02,\n",
      "         -2.1326e+00, -7.4892e-01, -5.1402e-01, -1.1667e+00, -7.5620e-02,\n",
      "         -8.6860e-01, -7.8701e-01, -9.2052e-01, -5.5790e-01, -8.2176e-02,\n",
      "         -7.4443e-01, -4.0922e-01, -9.0106e-01, -7.7496e-01, -1.4009e+00,\n",
      "         -1.2421e+00, -4.4056e-01,  1.0296e+00,  5.8492e-01,  3.2777e-01,\n",
      "         -9.5231e-01, -6.4501e-01, -1.6471e-01,  6.8691e-01, -2.5080e-01,\n",
      "         -4.8591e-01, -1.0867e-01,  4.0922e-01,  2.4100e-01,  1.2744e+00,\n",
      "         -1.9783e-02,  5.1926e-01, -1.3756e+00, -1.3770e+00, -1.2188e+00,\n",
      "         -1.3813e+00, -1.7815e+00, -1.0421e+00, -1.5683e+00, -5.7727e-01,\n",
      "         -1.2355e+00, -1.1207e+00, -1.2521e+00, -1.4540e+00, -1.5526e+00,\n",
      "         -1.5141e+00, -1.6537e+00, -2.2046e+00, -1.3994e+00, -5.0910e-01,\n",
      "         -3.8004e-01, -8.8016e-01, -1.6452e+00, -1.2116e+00, -1.3334e+00,\n",
      "          4.2603e-01,  1.6936e+00, -9.0126e-01, -3.7071e-01,  1.7145e-02,\n",
      "          2.5850e-02, -2.6556e-01, -7.5091e-02,  3.2555e-01,  7.4776e-02,\n",
      "          3.2068e-01,  6.0566e-01,  2.9574e-01,  6.3588e-01,  6.6941e-01,\n",
      "         -2.0856e-01, -1.1444e-01, -3.5474e-01,  6.1863e-01, -3.4114e-01,\n",
      "          1.4819e-02,  6.7350e-01,  4.3769e-01,  3.5838e-01,  3.8187e-01,\n",
      "         -6.8682e-01,  4.0407e-02,  3.5333e-02,  5.3574e-01,  6.1729e-01,\n",
      "          5.1614e-01, -1.1762e-01,  5.1620e-01,  2.4238e-02,  3.7983e-01,\n",
      "          4.9758e-01,  5.7163e-01,  2.5900e-01,  2.4198e-01,  5.7779e-01,\n",
      "         -7.1055e-01,  2.4399e-01,  5.7347e-01,  4.6724e-01, -8.2166e-01,\n",
      "          6.7061e-01,  1.9506e-01,  1.5961e-01,  1.8134e-01,  6.9438e-01,\n",
      "          1.7051e-01,  3.6183e-01,  4.8564e-01,  6.4617e-01,  2.1641e-01,\n",
      "          3.1593e-01, -6.8043e-02,  8.1003e-01,  1.4649e+00,  5.4409e-01,\n",
      "          2.8791e-01,  4.1902e-01,  5.7164e-01, -2.3254e-01, -5.9966e-02,\n",
      "          4.5608e-01,  7.1656e-03,  2.8171e-01, -3.7120e-01,  8.2868e-01,\n",
      "          1.9466e-01, -3.4073e-01,  1.5611e-01,  8.5500e-01,  2.9818e-01,\n",
      "          5.2570e-01,  1.9547e-01,  9.1846e-01, -2.5589e-01, -3.0882e-01,\n",
      "         -3.7585e-02,  5.6749e-01,  6.3947e-01, -1.1809e-01,  6.4872e-01,\n",
      "          7.9726e-01,  1.6744e-01,  4.6825e-01,  7.7850e-01, -1.8596e-01,\n",
      "          5.9437e-01, -4.4681e-02,  2.6189e-01,  4.9543e-01, -2.5393e-01,\n",
      "          5.6761e-01,  3.6635e-01,  6.9315e-02,  8.2455e-01,  4.7660e-01,\n",
      "          6.7019e-01,  6.5082e-01, -6.6104e-01,  6.8212e-01,  1.0893e+00,\n",
      "         -4.8246e-01,  5.0163e-01,  3.1570e-01, -3.2254e-03,  2.3776e-01,\n",
      "         -2.7567e-01, -5.3582e-01, -2.1770e-01,  4.6008e-01,  5.6306e-01,\n",
      "          6.2492e-01,  1.2187e-01,  3.3925e-01, -4.1561e-02, -2.2033e-01,\n",
      "         -5.6000e-01, -8.0514e-01, -3.7199e-01,  5.8466e-01, -8.1414e-01,\n",
      "         -6.5062e-01, -8.7191e-01, -6.8312e-01, -1.3053e+00, -3.1569e-01,\n",
      "         -1.1750e-01,  1.0370e+00,  1.0759e+00,  4.9018e-02,  5.9040e-01,\n",
      "          1.2840e+00,  8.0208e-02,  1.7170e-03, -3.7380e-01, -1.2972e+00,\n",
      "         -7.9316e-01, -9.9024e-01, -3.8351e-02, -9.4560e-01, -1.0187e+00,\n",
      "         -8.3867e-01, -8.7676e-01, -1.4525e+00, -2.3891e-01, -1.7768e-01,\n",
      "         -2.1715e+00, -8.0366e-01, -9.9156e-01, -4.3740e-01, -1.5885e+00,\n",
      "         -1.1768e+00, -2.1797e-01, -1.0114e+00, -1.1883e+00, -6.6305e-01,\n",
      "          3.3684e-02, -2.2481e-01, -5.8601e-01, -8.5715e-02,  5.4664e-01,\n",
      "         -7.3849e-01, -7.6291e-01, -1.1668e+00, -1.4109e+00, -8.1135e-01,\n",
      "         -1.5672e+00, -1.1634e+00, -1.4314e+00, -1.7554e+00, -1.6838e+00,\n",
      "         -1.6509e+00, -1.4180e+00,  6.2574e-02,  1.1291e-01, -3.6963e-01,\n",
      "          5.1681e-02, -1.8307e-01, -7.3346e-02,  3.4142e-01, -3.8051e-01,\n",
      "         -8.2211e-01, -1.6408e+00,  7.2538e-02,  7.4010e-01, -1.2744e+00,\n",
      "         -5.2491e-01,  5.9811e-01, -2.4999e-01, -1.3919e+00, -5.8177e-01,\n",
      "          6.4146e-01, -8.6079e-01, -1.6802e+00,  1.6698e-01, -1.1488e+00,\n",
      "         -9.6307e-01, -2.5132e+00, -1.4605e+00, -9.3626e-01, -1.0570e+00,\n",
      "          4.0902e-01,  9.7117e-01, -6.1179e-02,  3.5241e-01,  1.6006e-01,\n",
      "         -3.6530e-02,  4.0847e-01, -1.0528e-01,  4.3276e-01, -6.3090e-01,\n",
      "         -9.9350e-01, -1.5472e+00, -7.7325e-01, -1.3597e+00, -9.3616e-01,\n",
      "         -8.8030e-01, -3.6357e-01, -4.0993e-01,  1.6574e-02, -3.3175e-01,\n",
      "         -1.1199e+00, -1.1679e+00,  2.1333e-01, -3.5021e-01, -7.5697e-01,\n",
      "          3.3505e-01, -6.1781e-01, -3.5555e-01, -6.9205e-01, -7.9532e-01,\n",
      "         -5.9664e-01, -1.0161e+00, -1.0359e+00, -1.1021e+00, -2.6240e-01,\n",
      "          6.8680e-01, -3.1031e-02, -1.5263e+00, -1.4971e+00, -1.2868e-01,\n",
      "          5.5152e-01, -9.8942e-01, -6.7243e-01,  4.5435e-01,  5.5642e-02,\n",
      "         -7.8288e-01,  3.2608e-01,  3.2683e-02, -1.9855e+00, -1.8189e+00,\n",
      "         -8.8682e-01, -3.1034e-02, -1.8614e-01, -2.0704e-01,  1.1960e+00,\n",
      "         -2.8198e-01,  2.9534e-01,  2.2605e+00,  5.7824e-01,  3.6170e-01,\n",
      "          6.9688e-01, -6.0771e-01,  3.1665e-01,  7.1600e-02,  1.0237e+00,\n",
      "          5.2744e-01,  1.1795e+00, -2.4608e-01,  9.6445e-02, -2.4045e-01,\n",
      "         -9.3420e-01, -7.8523e-02,  1.5671e+00,  1.8181e+00,  3.4728e-01,\n",
      "         -7.0173e-01,  2.6523e-01, -6.0334e-02,  5.1304e-01,  3.9821e-01,\n",
      "          1.2401e+00, -5.4426e-01, -1.0580e-01,  5.8972e-01,  2.6498e-01,\n",
      "          8.2455e-01,  5.6106e-01,  1.2702e-01, -7.0222e-01, -6.8019e-01,\n",
      "          1.0990e-02,  2.5612e-01,  1.2441e+00,  1.0973e+00, -4.1306e-01,\n",
      "         -5.5708e-01,  6.2707e-01,  4.5272e-01, -3.7650e-01, -2.7514e-01,\n",
      "          5.8371e-01,  1.4203e+00,  1.0436e+00, -1.0926e-01,  5.2942e-01,\n",
      "         -4.9786e-01,  5.2703e-01,  1.4903e+00,  2.6183e+00,  1.0068e+00,\n",
      "         -4.0235e-01, -1.2180e+00, -2.7161e-01, -7.0590e-02,  1.6119e+00,\n",
      "          1.1645e+00,  7.4837e-01,  3.6863e-01,  9.6675e-01, -4.0557e-01,\n",
      "          3.1184e-01,  2.2212e-01,  5.2509e-01,  6.3456e-01,  6.3907e-01,\n",
      "          3.8827e-02,  2.0647e-01,  1.6019e-01, -8.7440e-01, -1.4598e+00,\n",
      "         -2.0526e-01, -4.6663e-01,  1.1290e+00,  1.6528e+00,  1.1263e+00,\n",
      "          1.9354e-01,  1.0330e+00,  6.0901e-01, -1.0364e+00,  1.1250e+00,\n",
      "         -1.0257e+00,  3.4963e-02, -5.5274e-01, -6.6571e-01,  1.0533e+00,\n",
      "         -1.6289e+00,  4.1260e-01,  1.2734e+00,  5.2191e-01,  1.3231e+00,\n",
      "          1.3470e+00,  1.0840e+00,  7.3523e-01,  4.7631e-01,  2.1077e-01,\n",
      "         -8.6855e-01, -9.4382e-01,  8.3002e-01,  1.6277e-01,  9.0806e-01,\n",
      "          1.6347e+00,  2.2341e-01,  2.0476e-01,  1.2934e+00,  5.1658e-01,\n",
      "         -7.7037e-01,  7.2997e-01,  6.5612e-01,  1.5743e+00,  1.5711e-01,\n",
      "         -7.2365e-01, -1.3918e-02, -4.5759e-01,  3.4963e-01,  5.1354e-02,\n",
      "          9.6703e-01,  2.5416e-01,  1.4687e-01, -1.1273e+00,  7.3303e-01,\n",
      "         -2.0077e-01, -4.0161e-01, -5.0467e-01,  8.8010e-02,  1.1712e+00,\n",
      "         -9.3468e-01,  1.3739e+00,  8.6721e-01,  9.7837e-01,  6.3280e-01,\n",
      "          7.8858e-01,  2.9517e-01, -1.8836e+00, -1.4216e+00, -2.3229e-01,\n",
      "         -2.2045e-01,  8.5617e-02,  6.6839e-01, -2.5685e-01, -1.3561e+00,\n",
      "         -5.4883e-01,  2.4629e-01,  3.5654e-01,  9.0908e-01,  8.0603e-01,\n",
      "          1.4929e-01,  2.5429e-02,  1.0149e+00, -2.1897e-01, -1.2911e+00,\n",
      "         -7.9020e-01,  2.6655e-01,  1.0328e+00,  5.6198e-01, -3.6299e-01,\n",
      "          9.7287e-01,  3.0279e-03,  8.4783e-01, -8.2117e-01,  4.9873e-01,\n",
      "         -1.0487e-01, -9.6343e-01,  1.0784e+00,  4.3982e-01,  3.6208e-03,\n",
      "         -5.8619e-02, -2.8607e-01,  7.5067e-01,  3.8769e-01,  9.5658e-01,\n",
      "          4.8591e-01, -5.9194e-01,  1.5934e+00,  1.1464e+00,  1.1397e+00,\n",
      "         -4.6484e-01,  2.0263e-01, -5.1422e-01,  4.6246e-01, -1.0227e-01,\n",
      "         -5.8132e-01,  1.0914e+00, -6.6079e-01, -5.8264e-01,  8.4161e-01,\n",
      "          2.2011e+00, -1.4199e-01, -1.9647e-01, -4.5297e-01,  5.4242e-01,\n",
      "          4.7435e-01,  1.4579e+00, -4.2986e-01,  2.1950e-01, -4.9556e-02,\n",
      "          7.8674e-01,  8.2995e-01, -4.4215e-01,  5.5001e-01,  2.6610e-01,\n",
      "          2.7482e-01,  1.2681e+00,  5.7299e-01,  1.8470e+00,  9.3421e-01,\n",
      "          7.7493e-01,  7.6244e-01,  8.7723e-02,  4.4967e-01, -1.0040e-01,\n",
      "         -1.1849e+00,  1.1145e+00, -2.4415e-01, -1.0265e+00,  2.2637e-01,\n",
      "         -3.3212e-01,  1.0410e+00,  7.0357e-01,  1.1877e+00,  3.5751e-02,\n",
      "          5.3308e-01,  1.1502e+00,  1.1958e+00,  6.1795e-01,  2.3017e-01,\n",
      "         -1.6242e+00,  1.0233e+00, -3.8002e-01,  1.2329e+00,  9.1020e-01,\n",
      "         -1.1066e+00,  6.5623e-01,  5.6257e-01, -5.6108e-01, -1.4803e+00,\n",
      "          8.6912e-01,  2.1573e-01,  6.4750e-01,  8.6091e-01,  6.2001e-02,\n",
      "          4.7644e-01,  7.3445e-02, -1.1904e-01, -7.3707e-02,  6.8438e-01,\n",
      "         -2.8225e-02, -1.1726e+00, -1.7488e-01, -7.9618e-01,  8.7901e-01,\n",
      "          3.2994e-01,  1.4702e+00,  2.0638e-01, -1.0163e+00, -3.6797e-01,\n",
      "          5.6828e-01, -1.0130e-01, -3.3755e-01,  5.7960e-01,  1.5470e+00,\n",
      "         -7.4543e-01,  1.6927e+00,  1.1748e+00,  1.0261e+00,  3.4740e-01,\n",
      "          9.9297e-01,  3.4725e-01, -6.3601e-01,  8.9082e-02,  6.2263e-01,\n",
      "         -1.1404e+00,  6.6130e-02, -1.5488e+00, -2.8796e-01, -8.4034e-01,\n",
      "         -5.7160e-01,  6.7350e-01,  9.1033e-01,  5.6564e-01, -8.6931e-01,\n",
      "          1.1608e+00,  1.5413e+00,  4.1731e-01, -4.6026e-01,  2.9697e-01,\n",
      "          1.8323e+00, -4.2540e-01, -2.3726e-01,  4.4773e-01,  6.1266e-01,\n",
      "         -5.8361e-01, -1.1070e-01,  3.8409e-01,  8.0039e-01,  1.9824e-01,\n",
      "          9.1736e-01,  1.1396e+00,  8.3942e-02, -7.3355e-01,  2.3828e-01,\n",
      "         -7.0380e-01,  4.2836e-01, -8.0309e-01, -2.8536e-01,  8.9302e-01,\n",
      "          6.4929e-01,  1.8069e-01,  1.5556e+00,  3.6383e-01, -8.2944e-01,\n",
      "          1.5108e+00, -6.5788e-01, -1.8238e-01,  1.4937e+00, -5.0328e-01,\n",
      "          4.1037e-01,  2.2807e+00, -6.7082e-01,  1.7662e+00, -1.4057e+00,\n",
      "         -9.4761e-02, -3.2890e-01,  5.9813e-01,  9.0345e-01,  2.5243e-01,\n",
      "          9.5595e-01, -3.0827e-01,  1.2747e-01,  1.7963e-01,  6.3358e-01,\n",
      "          2.9837e-02,  1.8673e-01,  5.6024e-01,  8.2708e-01,  1.5486e+00,\n",
      "          7.2681e-01, -1.2084e-01,  5.9616e-02,  6.0333e-01,  8.0198e-01,\n",
      "         -4.9314e-01,  1.1488e+00, -5.7808e-02,  1.3642e+00, -4.3128e-01,\n",
      "         -7.4281e-03,  1.0077e+00,  2.8510e-01,  4.4590e-01,  9.4528e-01,\n",
      "          7.9931e-01,  5.4349e-01,  6.3621e-01, -6.7268e-01,  1.0590e+00,\n",
      "          6.3460e-01,  6.8766e-01,  1.6632e+00,  8.1520e-01,  9.4596e-01,\n",
      "          4.8011e-01,  2.6544e-01,  4.2682e-01,  1.3565e+00, -8.7009e-01,\n",
      "         -1.1151e+00, -8.3822e-01,  9.9528e-01,  9.7638e-01,  1.7960e+00,\n",
      "          5.1232e-01,  4.6486e-01,  1.2601e+00,  2.5539e-01, -3.0955e-01,\n",
      "          8.8790e-01,  1.2765e+00,  1.6407e+00,  1.1171e+00,  4.4369e-01,\n",
      "         -2.4408e-01,  5.1233e-01,  8.4886e-01, -1.1586e+00,  4.6508e-01,\n",
      "         -9.9488e-01,  9.0455e-02, -1.1694e+00, -1.0343e+00,  1.3609e+00,\n",
      "          1.2689e+00,  1.6250e-01,  1.7491e-01,  1.0062e+00,  1.6780e-01,\n",
      "         -6.2666e-01,  1.0132e+00, -1.6381e-01,  1.7744e+00, -9.7617e-01,\n",
      "         -1.4018e-01,  4.0655e-01, -1.2974e+00,  1.7705e+00,  3.3261e-01,\n",
      "         -1.6901e+00, -9.5236e-01,  3.0010e-01,  1.1170e+00,  7.9092e-01,\n",
      "         -5.3404e-01,  4.2158e-01,  1.0357e+00,  1.3322e+00, -5.7361e-01,\n",
      "          1.0454e+00,  5.6276e-01, -7.4048e-01, -8.9560e-01, -1.5802e-02,\n",
      "          7.5360e-01,  1.4664e+00,  1.6544e+00,  9.7309e-01, -6.7731e-01,\n",
      "          1.4818e+00,  6.7223e-02,  5.6963e-01,  7.3666e-01,  7.3426e-01,\n",
      "          1.6808e+00,  8.6273e-01, -1.9557e-01,  2.4843e-01,  1.0486e+00,\n",
      "          1.4080e+00,  1.3257e+00,  1.8547e+00, -8.9294e-01, -4.1864e-01,\n",
      "          1.0010e+00, -6.3577e-01, -1.0054e-01, -4.9621e-01,  9.9951e-01,\n",
      "          6.1491e-02,  1.6258e+00,  7.1151e-01, -3.4655e-01, -4.6369e-01,\n",
      "          7.3356e-01,  1.0662e-01,  1.1992e-02,  1.6111e+00, -4.6276e-01,\n",
      "          1.0217e+00, -1.3787e+00,  9.5733e-01, -1.0046e+00, -2.0710e+00,\n",
      "          1.5932e-01,  1.8915e+00, -1.7021e-01, -4.0044e-01,  1.5171e+00,\n",
      "          7.8624e-01, -5.6439e-01,  1.1338e+00,  1.3747e+00,  2.4665e-01,\n",
      "          3.3425e-01, -2.2395e-01, -5.9313e-02, -8.8473e-01,  1.0351e-01,\n",
      "         -7.1909e-01,  2.4599e-01,  1.0225e+00,  3.3301e-02, -7.0777e-01,\n",
      "         -7.2415e-01,  1.5270e+00,  6.5049e-01,  2.2635e+00,  2.2624e+00,\n",
      "         -1.2027e+00, -3.8225e-01,  1.4335e+00,  3.7058e-01,  9.3805e-01,\n",
      "          1.1409e-02, -6.4208e-01,  1.4195e+00, -8.4493e-01,  1.0573e+00,\n",
      "          1.5041e+00,  7.9439e-01,  6.4519e-01, -3.7653e-01, -1.9444e+00,\n",
      "         -6.3156e-01,  9.6644e-02,  1.4870e-01,  5.4502e-01,  2.1634e-01,\n",
      "         -4.8766e-02,  7.7724e-01, -8.4768e-01,  6.2178e-01, -1.7088e-01,\n",
      "         -8.1189e-01, -7.7481e-01, -5.6520e-01, -3.8398e-02,  1.4808e+00,\n",
      "         -4.3465e-01,  2.2095e-01,  2.8755e-01, -1.5767e+00,  1.3453e-01,\n",
      "         -4.4149e-01,  3.3421e-01,  3.3348e-01,  4.0980e-03, -1.6548e-01,\n",
      "         -2.3070e-01, -7.2148e-01,  9.1480e-02,  1.9392e-01, -8.6501e-01,\n",
      "         -6.9611e-01, -1.4441e+00,  5.4446e-01,  9.0243e-01, -4.9560e-01,\n",
      "          1.4546e-01, -3.5968e-01, -7.2009e-01,  1.9001e-02,  6.8322e-01,\n",
      "         -8.7454e-03, -2.5100e-01, -4.3117e-01,  2.1374e-01, -7.9102e-01,\n",
      "          4.0711e-01,  6.5661e-01, -3.1920e-01, -8.2971e-01, -1.3476e+00,\n",
      "          5.1202e-02,  6.0334e-01, -4.1581e-01,  9.9526e-01,  4.0718e-02,\n",
      "         -3.2554e-02,  9.5097e-01, -7.8653e-02, -3.4362e-01, -1.9679e+00,\n",
      "          1.1220e+00, -1.4566e+00,  4.8155e-01,  2.7179e-01, -4.9719e-01,\n",
      "         -6.9267e-01, -5.9268e-02,  5.3879e-01, -4.8900e-01, -8.3164e-01,\n",
      "         -1.4167e+00, -2.4871e+00,  1.5209e+00, -1.8333e-01, -8.7414e-01,\n",
      "         -1.5157e-01, -1.3880e+00, -8.4934e-01, -1.8370e+00, -8.3315e-01,\n",
      "         -5.3221e-01,  2.1869e-01, -4.6024e-01,  1.3271e+00,  1.1347e+00]],\n",
      "       grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(data)\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "- Use the model prediction to calculate the error (`loss`)\n",
    "- Next step is to backpropagate this error through the networ\n",
    "- Back propagation is kicked off when we call `.backward()` on the error tensor\n",
    "- Autograd then calculates ad stores the gradients for eaeh model parameter in the parameter's `.grad` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-496.0020, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward()\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize\n",
    "- Next step is to load an optimizer, in this case SGD with a learning rate of 0.01 and `momentum` of 0.9\n",
    "- Register all the parameters of the model in the optimizer\n",
    "- SGD = Stochastic Gradient Descent\n",
    "- Momentum or SGD with momentum is method which helps accelerate gradients vectors in the right directions, thus leading to faster converging. Specifically it helps the model exit the local min/max to find the absolute min/max\n",
    "- Momentum = data from exponentially weighed averages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "\n",
    "print(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step\n",
    "- Finally, call `.step()` to initiate gradient descent (next epoch)\n",
    "- The optimizer adjusts each parameter by its gradient stored in `.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "optim.step()\n",
    "\n",
    "print(optim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differentiation in Autograd\n",
    "- How autograd collects gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensor a and b\n",
    "- We create 2 tensors `a` and `b` with `requires_grad=True`\n",
    "- This signal to `autograd` that every operation on them should be tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tensor Q from a and b\n",
    "- We create another tensor `Q` from `a` and `b`\n",
    "- Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Q into a scalar\n",
    "- Lets assume `a` and `b` to be parameters of an NN, and `Q` to be the error. In NN training, we want gradients of the error with respect to (w.r.t.) parameters\n",
    "    + dQ/da = 9a**2\n",
    "    + dQ/db = -2b\n",
    "- When we call `.backward()` on `Q`, autograd calculates these gradients and stores them in the respective tensors' `.grad` attribute\n",
    "- We need to explicitly pass a `gradient` argument in `Q.backward()` because it is a vector\n",
    "- `gradient` is a tensor of the same shape as `Q`, and it represents the gradient of Q w.r.t itself\n",
    "    + dQ/dQ = 1\n",
    "- Equivalently, we can also aggregate Q into a scalar and call backward implicitly, like `Q.sum().backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1.])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deposite gradient\n",
    "- Gradients are now deposited in `a.grad` and `b.grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational graph (DAG)\n",
    "- Conceptualy, autograd keeps a record of data (tensors) & all executed operations (along with the resulting new tensors) in a directed acyclic graph (DAG) consisting of Function objects\n",
    "- In this DAG, leaves are the input tensors, roots are the ouput tensors\n",
    "- By tracing this graph from roots to leaves, you can automatically compute the gradients using the chain rule\n",
    "- In a `forward pass`, autograd does 2 things simultaneously:\n",
    "    + Run the requested operation to compute a resulting tensor, and\n",
    "    + Maintain the operation's gradient function in the DAG\n",
    "- The `backward pass` kicks off when `.backward()` is called on the DAG root, `autograd` then\n",
    "    + Computes the gradients from each `.grad_fn`\n",
    "    + Accumulates them in the respective tensor's `.grad` attribute, and\n",
    "    + Using the chain rule, propagates all the way to the leaf tensors\n",
    "- Below is a visual representation of the DAG in our example. In the graph, arrows are in the direction of the forward pass. The nodes represent the backward functions of each operation in the forward pass. The leaf nodes in blue represent of leaf tensors `a` and `b`\n",
    "\n",
    "        a\n",
    "        |\n",
    "        v\n",
    "    PowBackward()       b\n",
    "        |               |\n",
    "        v               v\n",
    "    MulBackward()   PowBackward()\n",
    "        \\               /\n",
    "         \\             /\n",
    "           SubBacward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "- DAGs are dynamic in PyTorch\n",
    "- Important thing to note is that the graph is recreated from scratch\n",
    "- After each `.backward()` call, autograd starts populating a new graph\n",
    "- This is exactly what allows you to use control flow statements in your model\n",
    "- You can change the shape, size, and operations at eveery iteration if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exclusion from the DAG\n",
    "- `torch.autograd` tracks operations on all tensors which have their `requires_grad` flag set to `True`\n",
    "- For tensors tht don't require gradients, setting this attribute to `False` excludes it from the gradient computation DAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Requirements\n",
    "- The output tensor of an operation will require gradients even if only a single input tensor has `requires_grad=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does `a` require gradients? : False\n",
      "Does `b` require gradients?: True\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 5)\n",
    "y = torch.rand(5, 5)\n",
    "z = torch.rand((5, 5), requires_grad=True) \n",
    "\n",
    "a = x + y\n",
    "print(f\"Does `a` require gradients? : {a.requires_grad}\")\n",
    "b = x + z\n",
    "print(f\"Does `b` require gradients?: {b.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Parameters\n",
    "- In a NN, parameters tht don't compute gradients are usually called frozen parameters\n",
    "- It is useful to \"freeze\" part of your model if you know in advance that you won't need the gradients of those parameters (offer performance benefits by reducing autograd computation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tunning\n",
    "- In fine tunning, we freeze most of the model and typically only modify the classifier layers to make predictions on new labels\n",
    "- Example as below using resnet18 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "\n",
    "model1 = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# Freeze all the parameters in the network\n",
    "for param in model1.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets say we want to finetune the model on a new dataset with 10 labels\n",
    "- In resnet, the classifer is the last linear layer `model.fc`\n",
    "- we can replace it with a new linear layer (unfrozen by default) that acts as our classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now all the parameters in the model, except the parameters of `model.fc` are frozen\n",
    "- The only parameters that compute gradients are the weights and bias of `model.fc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model1.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Notice although we register all the parameters in the optimizer, the only parameters that are computing gradients ( and hence updated in the gradient descent) are the weights and bias of the classifier\n",
    "- The same exclusionary functionality is available as a context manager in `torch.no_grad()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vnpt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
