1. CNN

                                         C_1                                      S_1                                       C_2                                     S_2
    Input ----> 5x5 Convolution ----> Feature maps ----> 2x2 Subsampling ----> Feature maps ----> 5x5 Convolution ----> Feature maps ----> 2x2 Subsampling ----> Feature maps ----> n_1 ----> n_2 (output 0-9)
   (32x32)                             (28x28)                                  14x14                                       10x10                                   5x5                  fully connected
            \__________________________________________________________________________________________________________________________________________________________________/ \___________________________/
                                                                    Feature Extraction                                                                                                   classification

2. Convolution:
    - Convolution describes the process where a small patch, which designed to go around an image, analyze a portion of an input and eventually have the full data of the image.
         _______________
        /    _         /|
       /    | |       / /
      /     |_|      / /
     /              / / 
    /______________/ /
    |______________|/

    - Depth:
        + RGB = 3
        + Normal = 1
    - Height x Width x Depth
    - Patch is a small portion of the input image

3. Simple convolution layer:
    - Stride 1x1
    - Image 3x3x1       # (x)
    - Filter w 2x2x1    # (w)
    - Result in 1 number
    - Calculation:
        w * x = [0.1 0.5 0.3 0.4]*[1 2 4 5] = 0.1*1 + 0.5*2 + 0.3*4 + 0.4*5 = 4.3
    - Padding is when you add 0s around image data boundaries

4. Convolution layers:
    - Filters always extend the full depth of the input volume
    - Convolve the filter with the image "slide over the image partially, computing dot product"
    - When finish the dot product, convolve (slide) over all the spatial locations
    - Calculate activation map size from convolution layer:
        (height_img - height_fil + 1) x (width_img - width_fil +1) x depth = activation_map's height x width x depth
    - Depends on how many filters you have there will be the same amount of activation maps (depth)
        + Example: If we have 6 5x5 filters we will have activation map of depth 6

5. ConvNet:
    - ConvNet is a sequence of Convolution layers, interspersed with activation functions
    - 32x32x3 ----> CONV, RELU (6 5x5x3 filters) ----> 28x28x6 ----> CONV, RELU (10 5x5x6 filters) ----> 24x24x10 ----> ....

6. Max pooling:
    - Apply max pooling (subsampling) after each convolution layer
    - Do this to reuse the amount of information generated by convolution layer
    - Single depth slice 4x4 ----> Max pool with 2x2 filter of stride 2

7. Locally connected features
    - Fully connected neural net
        + Read the whole image and feed it as single input for next layer
    - Locally connected neural net          # CNN is this one
        + Use small size filter with a shared weight to look at all the images
        + Has smaller weight and more flexible

8. Simple CNN:
    - torch.nn.Conv2d(in_channels, out_channels, kernel_size)
        + in_channels = number of color (RGB)
        + out_channels = how many channels wanted to be generated
        + kernel_size = how big you want to see at once
    - torch.nn.MaxPool2d(kernel_size)
        + kernel_size = how big you want to see at once