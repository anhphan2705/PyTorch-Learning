1. Chain rule:
    - df/dx = df/dg * dg/dx
    - x_nodes = dL/dx = gradient of loss with respect to x
        + dL/dx = dL/dz * dz/dx
    - y_nodes = dL/dy = gradient of loss with respect to y
        + dL/dy = dL/dz * dz/dy 
    - z_nodes = dL/dz = output
    - f "local gradient"

2. Computational Graph:
    - y' = x * w
    - loss = (y' - y)^2 = (x*w - y)^2

    x =1
     \                          y' =1                  s =-1                      =1 
      > ------------- (*) ---------------- (-) ----------------- (^2) -------- loss
     /                  |       y =2 ------/ |                     |
    w =1                |                    |                     |
                        |                    |                     |
    |                   |                    |                     |
  d_loss/d_w       d_xw/d_w = x     (d_y' - y) / d_y' = 1     d_s^2/d_s = 2s
 

    1) Forward pass x= 1, y= 2, where w= 1      => ((1*1) - 2)^2 = 1
    2) Backward propagation 
        + d_loss/d_s = 2s = 2 *(-1) = -2
        + d_loss/d_y' = d_loss/d_s * d_s/d_y = (-2)*1 = -2
        + d_loss/d_w = d_loss/d_y' * d_y'/d_w = -2*x = -2 * 1 = -2
    - The computational graph moves like a cá»‰rcle.
    - It completes a forward pass and tranverse backward to compute gradient and recalculate.